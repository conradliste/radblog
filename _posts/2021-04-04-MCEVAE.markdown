---
layout: post
commments: true
title:  "Multi-Clustering Equivariant Variational Autoencoder (MCE-VAE)"
author: "Conrad Li"
date:   2021-04-04 02:33:41 -0500
tags: vae disentangled-reps
---

{:class="info"}
>  Learning disentangled representations of an agent's is essential for solving high-dimensional control problems. MCE-VAE introduces a hierarchical variational autoencoder that learns to separate invariant and equivariant components of the input space.

<!--excerpt-->

{: class="table-of-content"}
# Table of Contents
* TOC
{:toc}

## Notation

{: class="info"}
| Symbol | Definition | 
|------------------|-----------------------|
| x | Input vector |
| $$\mu_a$$ | Mean vector of any arbitrary variable $$a$$ |
| $$\sigma_a$$ | Variance vector of any arbitrary variable $$a$$|
| $$ w $$ | Weight vector for Latent mixture of Gaussians|
| $$z_{aug} $$ | Augmented latent vector |
| $$z_c $$ | Categorical latent vector | 
| $$z$$ | Variational latent vector |
| $$\tau$$ | Transformational latent vector |
| $$q_{z_{aug}}(x)$$ | **Deterministic** encoder: $$x \rightarrow z_{aug}$$ |
| $$q_{z_c}(z_{aug})$$ | **Probabilistic** encoder: $$z_{aug} \rightarrow \mu_{z_c}$$, $$\sigma_{z_c}$$, and $$w$$ |
| $$q_{z}(z_{aug})$$ | **Probabilistic** encoder: $$z_{aug} \rightarrow \mu_{z}$$ and $$\sigma_{z}$$
| $$q_{\tau}(z_{aug})$$ | **Probabilistic** encoder: $$z_{aug}$$ $$\rightarrow$$ $$\mu_{\tau}$$ and $$\sigma_{\tau}$$ |
| ---------- |  ---------|

## Motivations

Variational autoencoders (VAE) extend normal autoencoders by encoding each input
$$x$$ as a mean and variance in a low-dimensional latent
space. Although variational autoencoders (VAEs) have achieved impressive results
on image generation tasks and more recently 
[drug synthesis](https://pubs.acs.org/doi/10.1021/acscentsci.7b00572), crafting 
expressive latent spaces that can encode the structure of the input data remains
a difficult task. Much of research in VAEs has been directed toward creating 
*distentangled* latent spaces that encode each feature of $$x$$ into either narrowly
defined variables or separate dimension of $$z$$. More formally, a vector representation
is *disentangled*, if it can be decomposed into a number of subspaces, each one of which 
is compatible with, and can be transformed independently by a unique symmetry 
transformation ([Higgins](https://arxiv.org/pdf/1812.02230.pdf)). Multi-clustering
Equivariant Variational Autoencoder (MCE-VAE) attempts to craft a latent space
that learns invariant and equivariant latent representations of dynamic data.

## Methods


# Encoding the Input
For clarity, let us assume that the input dataset $$X = \{ x_0, x_1, ....x_N \}$$
are each $$ 28 \times 28 $$ images from the MNIST digit set.
The MCE-VAE consists of a four encoder functions. The first is $$q_{z_{aug}}(x)$$ is a 
deterministic encoder, implemented with ConvNets, that maps from the input $$x$$ to a single vector $$z_{z_{aug}}$$. From there, MCE-VAE factorizes the latent space into 3 latent variables, $$z_c$$, $$z$$, and $$\tau $$, each with its own encoder that maps the entire vector $$z_{aug}$$ to the parameters of a probability distribution. 

* $$z_c$$ encodes information about the class of the input (e.g. the digit)
* $$z$$ encodes variations in the input space such as color or bluriness
* $$\tau$$ encodes a SE-2 transformation that is applied to the reconstructed input

The latent variables $$z$$ and $$\tau$$ are represented as Gaussian distributions parameterized by a mean $$\mu$$ and $$\sigma$$. However, $$z_c$$ is represented as a [mixture of Gaussians](https://en.wikipedia.org/wiki/Mixture_distribution) parameterized by $$\mu$$, $$\sigma$$, and a weight vector $$w$$. In the paper, since the $$z_c$$
should encode the class of our input $x$, the dimensions of $$z_c$$ are set to equal
to the number of classes we expect in our input data.

In summary:

* $$q_{z_{aug}}(x)$$ maps from $$x$$ deterministically to $$z_{aug}$$

* $$q_{z_c}(z_{aug})$$ maps from $$z_{aug}$$ to a mixture of Gaussians parameterized by mean $$\mu_c$$, variance $$\sigma_c$$, and weight vector $$w$$

* $$q_{z}(z_{aug})$$ maps from $$z_{aug}$$ to a a Gaussian distribution parameterized by a mean $$\mu_z$$ and variance $$\sigma_z$$

* $$q_{\tau}(z_{aug})$$ maps from $$z_{aug}$$ to a Gaussian distribution parameterized by a mean $$\mu_{\tau}$$ and variance $$\sigma_{\tau}$$

# Decoding the latent variables
Now that we encoded our inputs into three factored latent variables, we now need a way to decode back to the input space while preserving equivariance and invariance of our factored latent variables. MCE-VAE has 2 decoder functions. The invariance decoder maps the catgorical latent variable $$z_c$$ and variantional latent variable $$z$$ back to a canonical reconstruction $$\tilde{x}$$. The equivariant decoder maps the canonical reconstuction $$\tilde{x}$$ and $$\tau$$ to the full reconstructed input $$\hat{x}$$ by applying an SE2 transformation to $$\tilde{x}$$. In this way, we learn the attributes of the input that are invariant to SE2 transformation by encoding them in to $$z_c$$ and $$z$$. We can then train MCE-VAE by maximizing the evidence lower bound [ELBO](https://en.wikipedia.org/wiki/Evidence_lower_bound). Note that since we have 3 probabilistic encoders, we also have 3 corresponding KL divergences in addition to the reconstruction loss.

The diagram below summarizes the MCE-VAE.


![mcevae](../../../assets/mcevae.png)





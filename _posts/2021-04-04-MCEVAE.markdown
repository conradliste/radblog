---
layout: post
title:  "Multi-Clustering Equivariant Variational AutoEncoder (MCE-VAE)"
date:   2021-04-04 02:33:41 -0500
categories: jekyll update
---

>  Learning disentangled representation of high-dimensional data is essential for solving more complex control problems. MCE-VAE introduces a hierarchical variational autoencoder that learns to separate invariant and equivariant components.

<!--excerpt-->

# Notation

| Symbol | Definition | 
|-----------------------------------------|
| x | input vector |
| dim(x) | Dimensions of any variable x |
| $$\mu_z$$ | Latent mean vector |
| $$\sigma_z$$ | Variance mean vector |
| $$z_{aug} $$ | Augmented latent vector |
| $$z_c $$ | Categorical latent vector | 
| $$z$$ | Variational latent vector |
| $$\tau$$ | Transformational latent vector |

# Motivations

Variational autoencoders (VAE) extend noraml autoencoders by encoding each input
$x$ as a mean and variance in a low-dimensional latent
space. Although variational autoencoders (VAEs) have achieved impressive results
on image generation tasks and more recently 
[drug synthesis](https://pubs.acs.org/doi/10.1021/acscentsci.7b00572), crafting 
expressive latent spaces that can encode the structure of the input data remains
a difficult task. Much of research in VAEs hase been directed toward creating 
*distentangled* latent spaces that encode each feature of $x$ into either narrowly
defined variables or separate dimension of $z$. More formally, a vector representation
is disentangled, if it can be decomposed into a number of subspaces, each one of which 
is compatible with, and can be transformed independently by a unique symmetry 
transformation [Higgins](https://arxiv.org/pdf/1812.02230.pdf). Multi-clustering
Equivariant Variational Autoencoder (MCE-VAE) attempts to craft a latent space
that learns invariant and equivariant latent representations of dynamic data.

# Methods
The 
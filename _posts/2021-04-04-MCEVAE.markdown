---
layout: post
commments: true
title:  "Multi-Clustering Equivariant Variational Autoencoder (MCE-VAE)"
author: "Conrad Li"
date:   2021-04-04 02:33:41 -0500
tags: vae disentangled-reps
---

{:class="info"}
>  Learning disentangled representations of an agent's is essential for solving high-dimensional control problems. MCE-VAE introduces a hierarchical variational autoencoder that learns to separate invariant and equivariant components of the input space.

<!--excerpt-->

{: class="table-of-content"}
# Table of Contents
* TOC
{:toc}

## Notation

{: class="info"}
| Symbol | Definition | 
|------------------|-----------------------|
| x | Input vector |
| $$\mu_a$$ | Mean vector of any arbitrary variable $$a$$ |
| $$\sigma_a$$ | Variance vector of any arbitrary variable $$a$$|
| $$ w $$ | Weight vector for Latent mixture of Gaussians|
| $$z_{aug} $$ | Augmented latent vector |
| $$z_c $$ | Categorical latent vector | 
| $$z$$ | Variational latent vector |
| $$\tau$$ | Transformational latent vector |
|$$\tilde{x}$$ | Invariant Canonical Reconstruction |
| $$\hat{x}$$ | Final Transformed Reconstruction|
| $$E(x)$$ | **Deterministic** encoder: $$x \rightarrow z_{aug}$$ |
| $$q(z_c \mid z_{aug})$$ | **Probabilistic** encoder: $$z_{aug} \rightarrow \mu_{z_c}$$, $$\sigma_{z_c}$$, and $$w$$ |
| $$q(z \mid z_{aug})$$ | **Probabilistic** encoder: $$z_{aug} \rightarrow \mu_{z}$$ and $$\sigma_{z}$$
| $$q(\tau \mid z_{aug})$$ | **Probabilistic** encoder: $$z_{aug}$$ $$\rightarrow$$ $$\mu_{\tau}$$ and $$\sigma_{\tau}$$ |
| $$d(\tilde{x} \mid z_c,\ z)$$ | **Probabilistic** decoder: $$z_c$$ and $$z$$ $$\rightarrow$$ $$\tilde{x}$$ |
| $$d(\hat{x} \mid \tau,\ \tilde{x})$$ | **Probabilistic** decoder: $$\tau$$ and $$\tilde{x}$$ $$\rightarrow$$ $$\hat{x}$$ |
| ---------- |  ---------|

## Motivations

Variational autoencoders (VAE) extend normal autoencoders by encoding each input
$$x$$ as a mean and variance in a low-dimensional latent
space. Although variational autoencoders (VAEs) have achieved impressive results
on image generation tasks and more recently 
[drug synthesis](https://pubs.acs.org/doi/10.1021/acscentsci.7b00572), crafting 
expressive latent spaces that can encode the structure of the input data remains
a difficult task. Much of research in VAEs has been directed toward creating 
*distentangled* latent spaces that encode each feature of $$x$$ into either narrowly
defined variables or separate dimension of $$z$$. More formally, a vector representation
is *disentangled*, if it can be decomposed into a number of subspaces, each one of which 
is compatible with, and can be transformed independently by a unique symmetry 
transformation ([Higgins](https://arxiv.org/pdf/1812.02230.pdf)). Multi-clustering
Equivariant Variational Autoencoder (MCE-VAE) attempts to craft a latent space
that learns features of the input data that are invariant to a user-specified Lie group transformation.
## Methods

# Encoding the Input
For clarity, let us assume that the input dataset $$X = \{ x_0, x_1, ....x_N \}$$
are each $$ 28 \times 28 $$ images from the MNIST digit set.
The MCE-VAE consists of a four encoder functions. The first is $$E(z_{aug})$$ is a 
deterministic encoder, implemented with ConvNets, that maps from the input $$x$$ to a single vector $$z_{z_{aug}}$$. From there, MCE-VAE factorizes the latent space into 3 latent variables, $$z_c$$, $$z$$, and $$\tau $$, each with its own encoder that maps the entire vector $$z_{aug}$$ to the parameters of a probability distribution. 

* The **categorical** latent variable, $$z_c$$, encodes information about the class of the input (e.g. the digit)
* The **semantic** latent variable, $$z$$,encodes variations in the input space such as color or bluriness
* The **transformation** latent variable, $$\tau$$, encodes a SE-2 transformation that is applied to the reconstructed input

The latent variables $$z$$ and $$\tau$$ are represented as Gaussian distributions parameterized by a mean $$\mu$$ and $$\sigma$$. However, $$z_c$$ is represented as a [mixture of Gaussians](https://en.wikipedia.org/wiki/Mixture_distribution) parameterized by $$\mu$$, $$\sigma$$, and a weight vector $$w$$. In the paper, since the $$z_c$$
should encode the class of our input $$x$$, the dimensions of $$z_c$$ are set to equal
to the number of classes we expect in our input data. Intuitively, each dimension of $$z_c$$ represents one class.

In summary:

* $$E(x)$$ maps from $$x$$ deterministically to $$z_{aug}$$

* $$q(z_c \mid z_{aug})$$ maps from $$z_{aug}$$ to a mixture of Gaussians parameterized by mean $$\mu_c$$, variance $$\sigma_c$$, and weight vector $$w$$

* $$q(z \mid z_{aug})$$ maps from $$z_{aug}$$ to a a Gaussian distribution parameterized by a mean $$\mu_z$$ and variance $$\sigma_z$$

* $$q(\tau \mid z_{aug})$$ maps from $$z_{aug}$$ to a Gaussian distribution parameterized by a mean $$\mu_{\tau}$$ and variance $$\sigma_{\tau}$$

# Decoding the Latent Variables
Now that we encoded our inputs into three factored latent variables, we now need a way to decode back to the input space while preserving equivariance and invariance of our factored latent variables. MCE-VAE has 2 decoder functions. The invariance decoder $$d(\tilde{x} \mid z_c,\ z)$$ maps the catgorical latent variable $$z_c$$ and variantional latent variable $$z$$ back to a invariant canonical reconstruction $$\tilde{x}$$. The equivariant decoder $$d(\hat{x} \mid \tau,\ \tilde{x})$$ maps the canonical reconstruction $$\tilde{x}$$ and $$\tau$$ to the full reconstructed input $$\hat{x}$$ by applying the SE2 transformation encoded by $$\tau$$ to $$\tilde{x}$$. In this way, we learn the attributes of the input that are invariant to SE2 transformation by encoding them in to $$z_c$$ and $$z$$. 


## Training MCE-VAE
Following other variational autoencoder architectures, we can train MCE-VAE by maximizing the evidence lower bound [ELBO](https://en.wikipedia.org/wiki/Evidence_lower_bound). Note that since we have 3 probabilistic encoders, we also have 3 corresponding KL divergences in addition to the reconstruction loss. The full ELBO is 

$$
ELBO = \mathbb{E}[log\ p(x \mid z_c, z, \tau)] - KL(q(z_c \mid x) \mid \mid p(z_c)) - KL(q(z \mid x) \mid \mid p(z)) - KL(q(\tau \mid x) \mid \mid p(\tau))
$$

* $$\mathbb{E}[log\ p(x \mid z_c, z, \tau)]$$ is the reconstruction term which is maxmized by minimizing a reconstruction loss.

* $$KL(q(z_c \mid x) \mid \mid p(z_c))$$ is the KL divergence between $$q(z_c \mid x)$$ and its Gaussian mixture prior $$p(z_c)$$

* $$KL(q(z \mid x) \mid \mid p(z)) $$ is the KL divergence between $$q(z \mid x)$$ and its Gaussian prior $$p(z)$$

* $$KL(q(\tau \mid x) \mid \mid p(\tau))$$ is the KL divergence between $$q(\tau \mid x)$$ and its Gaussian prior $$p(\tau)$$

## Diagram

The diagram below summarizes the MCE-VAE.


![mcevae](../../../assets/mcevae.png)




